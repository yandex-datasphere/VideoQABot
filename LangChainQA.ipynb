{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee149ac5",
   "metadata": {
    "cellId": "mes8twdaihp199vumyadbj",
    "execution_id": "dd6efd9c-99e4-4ce2-b68d-5a6249bf0a13"
   },
   "source": [
    "## Строим вопрос-ответного бота по технологии Retrieval-Augmented Generation на LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com) - это набирающая популярность библиотека для работы с большими языковыми моделями и для построения конвейеров обработки текстовых данных. В одной библиотеке присутствуют все элементы, которые помогут нам создать вопрос-ответного бота на наборе текстовых данных: вычисление эмбеддингов, запуск больших языковых моделей для генерации текста, поиск по ключу в векторных базах данных и т.д.\n",
    "\n",
    "Для начала, установим `langchain` и сопутствующие библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26382e7",
   "metadata": {
    "cellId": "yzdzyw1o8ujhmnrk4xly6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/89/b2/3b74b85356662637bfe3efbc6462ccb28227215fcf8e07b5e9a830f5c661/langchain-0.0.275-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.0.275-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lancedb\n",
      "  Obtaining dependency information for lancedb from https://files.pythonhosted.org/packages/0b/59/8800508af2c0afe0269278432ed5a7a8b03bebc75826ac211ee4053bf7a1/lancedb-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading lancedb-0.2.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting unstructured\n",
      "  Obtaining dependency information for unstructured from https://files.pythonhosted.org/packages/e3/97/478e5f01e8922acc140ee35adde4c5e6861f7a693fe57daafe53947e6602/unstructured-0.10.8-py3-none-any.whl.metadata\n",
      "  Downloading unstructured-0.10.8-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.6.0,>=0.5.7 from https://files.pythonhosted.org/packages/97/5f/e7cc90f36152810cab08b6c9c1125e8bcb9d76f8b3018d101b5f877b386c/dataclasses_json-0.5.14-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.21 from https://files.pythonhosted.org/packages/2b/cb/3525fb0d1bf144840c726345a107ad35998565a05e99d4bfec755c71ffd8/langsmith-0.0.27-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.0.27-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/83/8d/f65f8138365462ace54458a9e164f4b28ce1141361970190eef36bdef986/transformers-4.32.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.4.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pylance==0.6.5 (from lancedb)\n",
      "  Obtaining dependency information for pylance==0.6.5 from https://files.pythonhosted.org/packages/bf/71/acaace11900627025b64efac1b6496755b4000ec335837ed3940616d6e3f/pylance-0.6.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pylance-0.6.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting ratelimiter (from lancedb)\n",
      "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
      "Collecting retry (from lancedb)\n",
      "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
      "Collecting attr (from lancedb)\n",
      "  Downloading attr-0.3.2-py2.py3-none-any.whl (3.3 kB)\n",
      "Collecting semver>=3.0 (from lancedb)\n",
      "  Obtaining dependency information for semver>=3.0 from https://files.pythonhosted.org/packages/d4/5d/f2b4fe45886238c405ad177ca43911cb1459d08003004da5c27495eb4216/semver-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading semver-3.0.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting pyarrow>=10 (from pylance==0.6.5->lancedb)\n",
      "  Obtaining dependency information for pyarrow>=10 from https://files.pythonhosted.org/packages/a1/14/4ffed5e85b96f0c0ae9e026f940bf71ac7dfbfbffff9f3fe339e32bfce2c/pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.0.0)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /kernel/lib/python3.10/site-packages (from unstructured) (4.12.2)\n",
      "Collecting emoji (from unstructured)\n",
      "  Obtaining dependency information for emoji from https://files.pythonhosted.org/packages/96/c6/0114b2040a96561fd1b44c75df749bbd3c898bf8047fb5ce8d7590d2dee6/emoji-2.8.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /kernel/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /kernel/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (20.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/6c/f0/c17bbdb1e5f9dab29d44cade445135789f75f8f08ea2728d04493ea8412b/safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /kernel/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /kernel/lib/python3.10/site-packages (from retry->lancedb) (5.1.1)\n",
      "Collecting py<2.0.0,>=1.4.26 (from retry->lancedb)\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /kernel/lib/python3.10/site-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (2.4.7)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Downloading langchain-0.0.275-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lancedb-0.2.2-py3-none-any.whl (35 kB)\n",
      "Downloading pylance-0.6.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured-0.10.8-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.27-py3-none-any.whl (34 kB)\n",
      "Downloading semver-3.0.1-py3-none-any.whl (17 kB)\n",
      "Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125915 sha256=4c76632795f0e26c6c36b45c4873e663e45b682df7a8d8cb7d02049af212a587\n",
      "  Stored in directory: /tmp/xdg_cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, ratelimiter, filetype, attr, semver, python-magic, pyarrow, py, mypy-extensions, emoji, charset-normalizer, typing-inspect, retry, pylance, marshmallow, unstructured, langsmith, lancedb, huggingface-hub, dataclasses-json, transformers, langchain, sentence_transformers\n",
      "\u001b[33m  WARNING: The script filetype is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pysemver is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script unstructured-ingest is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 13.0.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attr-0.3.2 charset-normalizer-2.0.12 dataclasses-json-0.5.14 emoji-2.8.0 filetype-1.2.0 huggingface-hub-0.16.4 lancedb-0.2.2 langchain-0.0.275 langsmith-0.0.27 marshmallow-3.20.1 mypy-extensions-1.0.0 py-1.11.0 pyarrow-13.0.0 pylance-0.6.5 python-magic-0.4.27 ratelimiter-1.2.0.post0 retry-0.9.2 safetensors-0.3.3 semver-3.0.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.32.1 typing-inspect-0.9.0 unstructured-0.10.8\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain sentence_transformers lancedb unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e1a7f",
   "metadata": {
    "cellId": "01gt55s25xlq8thmgt142tx",
    "execution_id": "202bd627-2f9f-4492-928d-9d30336acd19"
   },
   "source": [
    "## Разбиваем документ на части\n",
    "\n",
    "Для работы retrival augmented generation нам необходимо по запросу найти наиболее релевантные фрагменты исходного текста, на основе которых будет формироваться ответ. Для этого нам надо разбить текст на такие фрагменты, по которым мы сможем вычислять эмбеддинг, и которые будут с запасом помещаться во входное окно используемой большой языковой модели.\n",
    "\n",
    "Для этого можно использовать механизмы фреймворка LangChain - например, `RecursiveCharacterTextSplitter`. Он разбивает текст на перекрывающиеся фрагменты по набору типовых разделителей - абзацы, переводы строк, разделители слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9d383f",
   "metadata": {
    "cellId": "503n864wu3akn1mjo318yj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.03s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "import langchain.document_loaders\n",
    "\n",
    "source_dir = \"/home/jupyter/mnt/s3/mclass/text\"\n",
    "\n",
    "loader = langchain.document_loaders.DirectoryLoader(source_dir,glob=\"*.txt\",show_progress=True,recursive=True)\n",
    "splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(chunk_size=512,chunk_overlap=50)\n",
    "fragments = splitter.create_documents([ x.page_content for x in loader.load()])\n",
    "len(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068ad70",
   "metadata": {
    "cellId": "xmlbo4mkcqt1zjx877mza1",
    "execution_id": "1a16dc51-9ba9-4725-875d-f9a085e1e587"
   },
   "source": [
    "## Вычисляем эмбеддинги для всех фрагментов\n",
    "\n",
    "Для вычисления эмбеддингов можно взять какую-нибудь модель из репозитория HuggingFace, с поддержкой русского языка. LangChain содержит свои встроенные средства работы с эмбеддингами, и поддерживает модели из HuggingFace: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "618d852c",
   "metadata": {
    "cellId": "lnsg67kshqfp494yccul07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)5f450/.gitattributes: 100%|██████████| 690/690 [00:00<00:00, 73.6kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 36.6kB/s]\n",
      "Downloading (…)/2_Dense/config.json: 100%|██████████| 114/114 [00:00<00:00, 103kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.58M/1.58M [00:01<00:00, 1.48MB/s]\n",
      "Downloading (…)966465f450/README.md: 100%|██████████| 2.38k/2.38k [00:00<00:00, 1.94MB/s]\n",
      "Downloading (…)6465f450/config.json: 100%|██████████| 556/556 [00:00<00:00, 495kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 102kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 539M/539M [00:42<00:00, 12.6MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 46.5kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 93.0kB/s]\n",
      "Downloading (…)5f450/tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 6.08MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 452/452 [00:00<00:00, 390kB/s]\n",
      "Downloading (…)966465f450/vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 9.34MB/s]\n",
      "Downloading (…)465f450/modules.json: 100%|██████████| 341/341 [00:00<00:00, 294kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = langchain.embeddings.HuggingFaceEmbeddings(model_name=\"distiluse-base-multilingual-cased-v1\")\n",
    "sample_vec = embeddings.embed_query(\"Hello, world!\")\n",
    "len(sample_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a9a5705",
   "metadata": {
    "cellId": "nfqfqhfm1o008d49oy4786s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "iam_token = \"t1.9euelZqUlZjJm5qRyJjOzJ2WmZ7OnO3rnpWalpOXl5iQzJqTyZeVzJaRl5vl8_dJTDlY-e8jFWFG_N3z9wl7Nlj57yMVYUb8zef1656VmpGUzIydyZHJyM3GjIqYnMqM7_zF656VmpGUzIydyZHJyM3GjIqYnMqM.BkHA7mr54HYTHGyb6frKXXGIfLuQOYB3S-yevqHpZzpp2CeKPavBaYGUFiKfNV-fNxfvxGoghwBfOUIU-NjdDw\"\n",
    "folder_id=\"b1gbicod0scglhd49qs0\"\n",
    "headers={ \n",
    "    \"Authorization\" : f\"Bearer {iam_token}\",\n",
    "    \"x-folder-id\" : folder_id\n",
    "}\n",
    "j = {\n",
    "  \"model\" : \"general\",\n",
    "  \"text\": \"Hello, world!\"\n",
    "}\n",
    "\n",
    "res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/embedding\",json=j,headers=headers)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f614d",
   "metadata": {
    "cellId": "q5znrcj1wan9h6ddvj28p",
    "execution_id": "dbfc733d-c9d0-4c6f-af7c-301b3d231364"
   },
   "source": [
    "## Cохраняем эмбеддинги  в векторную БД\n",
    "\n",
    "Для поиска нам нужно уметь быстро сопоставлять эмбеддинг запроса, и эмбеддинги всех фрагементов наших исходных материалов. Для этого используются векторные базы данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475fab93",
   "metadata": {
    "cellId": "5tmslotcscignkmh50g49p"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to deserialize variable 'db'. Run the following code to delete it:\n",
      "  del_datasphere_variables('db', 'embeddings', 'retriever', 'table')\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/state/state_protocol.py\", line 283, in _load_component\n",
      "    value = unpickler.load()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/lance/dataset.py\", line 71, in __init__\n",
      "    self._ds = _Dataset(\n",
      "ValueError: Dataset at path home/jupyter/work/resources/store/vector_index.lance/_versions/2.manifest was not found: Not found: home/jupyter/work/resources/store/vector_index.lance/_versions/2.manifest\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ml_kernel.state.state_protocol.KernelStateProtocol.DeserializationException: ['db', 'embeddings', 'retriever', 'table']\n",
      "Failed to deserialize variable 'db'. Run the following code to delete it:\n",
      "  del_datasphere_variables('db', 'embeddings', 'retriever', 'table')\n",
      "Traceback (most recent call last):\n",
      "  File \"/kernel/lib/python3.10/site-packages/ml_kernel/state/state_protocol.py\", line 283, in _load_component\n",
      "    value = unpickler.load()\n",
      "  File \"/home/jupyter/.local/lib/python3.10/site-packages/lance/dataset.py\", line 71, in __init__\n",
      "    self._ds = _Dataset(\n",
      "ValueError: Dataset at path home/jupyter/work/resources/store/vector_index.lance/_versions/2.manifest was not found: Not found: home/jupyter/work/resources/store/vector_index.lance/_versions/2.manifest\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "ml_kernel.state.state_protocol.KernelStateProtocol.DeserializationException: ['db', 'embeddings', 'retriever', 'table']\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import LanceDB\n",
    "import lancedb\n",
    "\n",
    "db_dir = \"../store\"\n",
    "\n",
    "db = lancedb.connect(db_dir)\n",
    "table = db.create_table(\n",
    "    \"vector_index\",\n",
    "    data=[\n",
    "        {\n",
    "            \"vector\": embeddings.embed_query(\"Hello World\"),\n",
    "            \"text\": \"Hello World\",\n",
    "            \"id\": \"1\",\n",
    "        }\n",
    "    ],\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcaa8e7d",
   "metadata": {
    "cellId": "j0bkb4g6igstk2umstio2j"
   },
   "outputs": [],
   "source": [
    "db = LanceDB.from_documents(fragments, embeddings, connection=table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4be3a5",
   "metadata": {
    "cellId": "fdty939nkuf433bv37l2cs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "и работать да какая проблема о том что у нас есть смотри ка для русского языка захотели мы сделать казахский язык захотели сделать английский язык захотели сделать еще 5 языков И в этот момент каждый такой язык стоит дорого потому что им надо знать людям потому что это проектная работа становится и по железу то есть мы начинаем держать несколько разных инсталляций каждый из которых требует же плюшек при этом всем известно как это работает трафик на новых сервисах у вас есть так вот так вот большой трафик и\n",
      "----------------------------------------\n",
      "на криптоинтах так далее организовать это в зуме достаточно сложно клинической среде постоянно есть конференции на которой ученые ездят обсуждают какие то идеи вот поэтому У нас есть возможность там как то работать удаленно но это всегда кейс бэкейс В зависимости от человека вот пока что показывает что Люди которые давно работают с большим опытом они в принципе могут работать удаленно но ожидать от например высших стажеров или Там наших разработчиков они будут эффективны на рынке Есть единичная история но\n",
      "----------------------------------------\n",
      "точно в санкт петербурге очевидно Но сейчас же очень много людей которые работают удаленно если я правильно понимаю Да на самом деле с тем как работать удаленно это вообще большой вопрос то есть есть там разные типы людей есть люди которые могут спокойно сидеть где то там на шри ланке и код решать задачки С исследованиями разве гораздо сложнее Потому что нужна доска Ну понятно да и носки нужно что то нарисовать и многие идеи про то что же поделать они там приходят на криптоинтах так далее организовать это\n",
      "----------------------------------------\n",
      "сервисов в нем Потому что это же очень здорово когда ты стартуешь сервис да у него там казалось бы там небольшая там база да там какое то количество пользователей но в масштабе облака вроде бы не очень много И ты вот ищешь А как как поменять в этом сервисе что добавить какие нужны фичи или что как всему должен быть интегрирован чтобы тот элемент нагнулся тогда чтобы чтобы он утонул в этом своем росте И когда находишь и ты видишь что да вот оно пошло рост рост попер Это же прям такой ни с чем не сравнимый\n"
     ]
    }
   ],
   "source": [
    "q=\"Почему стоит работать в Яндексе?\"\n",
    "\n",
    "res = db.similarity_search(q)\n",
    "for x in res:\n",
    "    print('-'*40)\n",
    "    print(x.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209d30d6",
   "metadata": {
    "cellId": "94wo2w5zglg0oyuubdoqlzc"
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "res = retriever.get_relevant_documents(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423a58c1",
   "metadata": {
    "cellId": "j17m1mbp5hgxbgxvx0xnz"
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "import requests\n",
    "\n",
    "class YandexLLM(langchain.llms.base.LLM):\n",
    "    iam_token: str\n",
    "    folder_id: str\n",
    "    max_tokens : int = 1500\n",
    "    temperature : float = 1\n",
    "    instruction_text : str = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"yagpt\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        req = {\n",
    "          \"model\": \"general\",\n",
    "          \"instruction_text\": self.instruction_text,\n",
    "          \"request_text\": prompt,\n",
    "          \"generation_options\": {\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature\n",
    "          }\n",
    "        }\n",
    "        res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/instruct\",\n",
    "          headers=\n",
    "            { \"Authorization\" : f\"Bearer {self.iam_token}\",\n",
    "              \"x-folder-id\" : self.folder_id }, json=req).json()\n",
    "        return res['result']['alternatives'][0]['text']\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"max_tokens\": self.max_tokens, \"temperature\" : self.temperature }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36716bd",
   "metadata": {
    "cellId": "17hb5txavf5iiscbggvvjg"
   },
   "outputs": [],
   "source": [
    "iam = \"t1.9euelZrIzZKXj5iUy82OlJWezpOQle3rnpWalpOXl5iQzJqTyZeVzJaRl5vl9PcabkZY-e80fUvD3fT3WhxEWPnvNH1Lw83n9euelZqOisfOyI2QkJuemJLGjZvOyu_8xeuelZqOisfOyI2QkJuemJLGjZvOyg.OTDSozh2X04QzrfbvcVxnjBBkxgrRQ_GIzY4TKfWEEut2VumQN1qBuyTWAl8REVdALeooZd_qa8iroTSB6jKDA\"\n",
    "\n",
    "instructions = \"\"\"\n",
    "Представь себе, что ты сотрудник Yandex Cloud. Твоя задача - подробно отвечать на все вопросы собеседника.\"\"\"\n",
    "\n",
    "llm = YandexLLM(iam_token=iam, folder_id=\"b1gbicod0scglhd49qs0\",\n",
    "                instruction_text = instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3000d8e0",
   "metadata": {
    "cellId": "4ek9wo0my135q1p79ismqh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Приветствую! Как сотрудник Yandex Cloud я действительно понимаю, насколько важно убедить людей в том, почему работать в Яндексе это блестящая идея. Вот несколько причин, по которым работа здесь может быть необыкновенно захватывающей:\\n\\n- Сотрудничество с одними из лучших в мире специалистов: Yandex является крупнейшей высокотехнологичной компанией в России, и мы сотрудничаем с наиболее опытными коллегами в своей отрасли. Наша команда зависит от профессионализма каждого отдельного члена команды, который развивается и улучшает свои навыки на ежедневной основе.\\n\\n- Инновации на каждом шагу: Все без исключения команды Яндекса работают над инновационными проектами, цель которых – развивать новые, захватывающие продукты и технологии, которые делают жизнь людей лучше. Специализированные подразделения Яндекса занимаются разработкой интеллектуальных интерфейсов и инструментов для нашей платформы хранения данных – Yandex Database. \"yandex/db\" – наша операционная система для баз данных, которая позволяет интегрировать базы данных с облаком Yandex Compute Cloud возможностью поддержки СУБД всех стандартов: ORACLE, MSSQL, NODEJS, HTTP, JDBC и многих других. Фактически она позволяет любому иметь мощное облако в руках – вы можете создаёт из EC2 машин систему управления данными любой степени сложности и масштаба, используя готовые решения или уникальные разработки компании. На платформе Yandex также есть пакетное хранилище данных Yandex Object Storage для кросс-платформенной работы как разработчиков, так и бизнеса. \"storage/bucket\" предоставляет разработчикам и владельцам данных на базе этих платформ инструменты для хранения, обработки и извлечения независимых данных предприятие продолжит оптимизацию бизнес-процессов посредством технологий, которые разрабатывают внешние партнеры.  VM Sandbox – это сервис, который способствует разработке инновационных приложений и обеспечивает безопасность при выполнении Операций с ИТ-ресурсами. Он обеспечивает полную независимость в формате песочницы на прошедшем или домашне сломанном гипервизоре, идеальном для экспериментов в проблемной среде или для виртуальной основной среды непосредственно облачного технологического процесса на основе средств, шаблонов, шаблонов и криптографского цифрового сертификата. Компания использует облачные технологии для роста в бизнесе. Мы помогаем нашим партнерам извлекать выгоду из использования современных технологий в любых отраслях и бизнес-сценариях. Они используют Yandex как безопасную структуру передачи данных и платформу для создания облачных сервисов, предлагаемых как внешним клиентам, так и самим партнерам, на регулярной оплате и тарифе. Таким образом, воображение наших сотрудников способно создать множество прорывных технологий – от AI до машинного обучения.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a500165",
   "metadata": {
    "cellId": "77y4w5s6seg08g1utincn7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Во-первых, Яндекс - это мировая компания с большими возможностями для развития и роста в профессиональном плане. В Яндекс работают известные мировые лидеры, которые создают инновационные продукты, преобразовывающие сферы человеческих знаний и деятельности. Здесь ты сможешь раскрыть свой потенциал и развить новые навыки, а также получить корпоративные знания и опыт.\\n\\nВо-вторых, в Яндексе ценят ответственный подход к работе и высокую степень профессионализма, создают все условия для эффективной работы и обучения сотрудников, которые готовы прийти на помощь в любых ситуациях.\\n\\nВ-третьих, благодаря облачным услугам и сервисам Яндекс, его клиенты по всему миру могут высоко оценить качество и возможности развития своих компаний, предоставляя бизнесу и пользователям продукта многие из самых сложных и инновационных решений в Интернете. Каждый день лично и во взаимодействии со своими коллегами ты становишься частью больших и масштабных проектов, участники и авторы которых работают над клиентскими сервисами, продуктами и проектами, делая жизнь людей лучше и проще, повышая качество и безопасность онлайн мира.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We prepare and run a custom Stuff chain with reordered docs as context.\n",
    "\n",
    "# Override prompts\n",
    "document_prompt = langchain.prompts.PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "stuff_prompt_override = \"\"\"Пожалуйста, посмотри на текст ниже и ответь на вопрос, используя информацию из этого текста.\n",
    "Текст:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Вопрос:\n",
    "{query}\"\"\"\n",
    "prompt = langchain.prompts.PromptTemplate(\n",
    "    template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "\n",
    "# Instantiate the chain\n",
    "llm_chain = langchain.chains.LLMChain(llm=llm, prompt=prompt)\n",
    "chain = langchain.chains.StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    ")\n",
    "chain.run(input_documents=res, query=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e38c691c",
   "metadata": {
    "cellId": "dhzsgivm5s9mpky1db082"
   },
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "reorderer = LongContextReorder()\n",
    "\n",
    "def answer(query,reorder=True):\n",
    "  results = retriever.get_relevant_documents(query)\n",
    "  if reorder:\n",
    "    results = reorderer.transform_documents(results)\n",
    "  return chain.run(input_documents=results, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96149ae4",
   "metadata": {
    "cellId": "is1wpwxhw1inxmwsqvixqa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Да, такая команда существует. Она отвечает за разработку и внедрение алгоритмов и машинному обучению. Они предоставляют гибкие решения для разработки, тестирования, развертывания и использования моделей машинного обучения. Команда ML также занимается совместной работой с другими командами продукта, такими как DevOps и аналитики данных.В ее обязанности входит обучение моделей данных.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Что ты можешь сказать об ML-команде Яндекс-облака?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a006ce0f",
   "metadata": {
    "cellId": "4byevx8504hx6nicfe2hia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ YaGPT: Конечно, стоит вопрос и я смогу помочь на него ответить и предоставлю всю возможную необходимую информацию:\n",
      "\n",
      "ML-команда Yandex.Cloud занимается разработкой технологий машинного обучения для всех облачных продуктов компании. Команда создает и совершенствует черные ящики, которые могут предсказывать новые сегменты данных и способны перестраиваться для новых запросов и моделей бизнеса\n",
      "Ответ бота: Команда Яндекс  Облака постоянно использует ML (машинное  обучение), совершенствуя продукты по мере востребованности. Сотрудники активно участвуют в исследуемых темах, имеют отличную квалификацию и способности решать сложные задачи.\n",
      "\n",
      "За прошедший год команда Яндекса запустила образовательные ресурсы нового уровня, задала новый стандарт презентации результатов научным исследованиям. Теперь результаты этих исследований доступны миллионам открытым доступом по решению Европейской комиссии, а также случайно образованной публичной группе специалистов в области машинного обучения.\n"
     ]
    }
   ],
   "source": [
    "def compare(q):\n",
    "    print(f\"Ответ YaGPT: {llm(q)}\")\n",
    "    print(f\"Ответ бота: {answer(q)}\")\n",
    "    \n",
    "compare(\"Что ты можешь сказать об ML-команде Яндекс-облака?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0293614",
   "metadata": {
    "cellId": "i35qdb5xtoohh4yt0ah15v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "369d3e82-763a-4bcd-b5cc-0cfe14f13f53",
  "notebookPath": "VideoQABot/LangChainQA.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
