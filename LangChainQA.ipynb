{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee149ac5",
   "metadata": {
    "cellId": "mes8twdaihp199vumyadbj",
    "execution_id": "dd6efd9c-99e4-4ce2-b68d-5a6249bf0a13"
   },
   "source": [
    "## Строим вопрос-ответного бота по технологии Retrieval-Augmented Generation на LangChain\n",
    "\n",
    "[LangChain](https://python.langchain.com) - это набирающая популярность библиотека для работы с большими языковыми моделями и для построения конвейеров обработки текстовых данных. В одной библиотеке присутствуют все элементы, которые помогут нам создать вопрос-ответного бота на наборе текстовых данных: вычисление эмбеддингов, запуск больших языковых моделей для генерации текста, поиск по ключу в векторных базах данных и т.д.\n",
    "\n",
    "Для начала, установим `langchain` и сопутствующие библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e26382e7",
   "metadata": {
    "cellId": "yzdzyw1o8ujhmnrk4xly6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script filetype is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script pysemver is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script unstructured-ingest is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 13.0.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain sentence_transformers lancedb unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e1a7f",
   "metadata": {
    "cellId": "01gt55s25xlq8thmgt142tx",
    "execution_id": "202bd627-2f9f-4492-928d-9d30336acd19"
   },
   "source": [
    "## Разбиваем документ на части\n",
    "\n",
    "Для работы retrival augmented generation нам необходимо по запросу найти наиболее релевантные фрагменты исходного текста, на основе которых будет формироваться ответ. Для этого нам надо разбить текст на такие фрагменты, по которым мы сможем вычислять эмбеддинг, и которые будут с запасом помещаться во входное окно используемой большой языковой модели.\n",
    "\n",
    "Для этого можно использовать механизмы фреймворка LangChain - например, `RecursiveCharacterTextSplitter`. Он разбивает текст на перекрывающиеся фрагменты по набору типовых разделителей - абзацы, переводы строк, разделители слов.\n",
    "\n",
    "> **ВАЖНО**: Перед выполнением ячейки не забудьте установить имя пользователя, которое вы использовали на предыдущем шаге.\n",
    "\n",
    "Размер `chunk_size` нужно выбирать исходя из нескольких показателей:\n",
    "* Допустимая длина контекста для эмбеддинг-модели. Yandex GPT Embeddings допускают 2048 токенов, в то время как многие открытые модели HuggingFace имеют длину контекста 512-1024 токена\n",
    "* Допустимый размер окна контекста большой языковой модели. Если мы хотим использовать в запросе top 3 результатов поиска, то 3 * chunk_size+prompt_size+response_size должно не превышать длины контекста модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c9d383f",
   "metadata": {
    "cellId": "503n864wu3akn1mjo318yj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s][nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "import langchain.document_loaders\n",
    "\n",
    "user = 'shwars'\n",
    "chunk_size = 1024\n",
    "chunk_overlap=50\n",
    "source_dir = f\"/home/jupyter/datasphere/s3/s3store/{user}/text\"\n",
    "\n",
    "loader = langchain.document_loaders.DirectoryLoader(source_dir,glob=\"*.txt\",show_progress=True,recursive=True)\n",
    "splitter = langchain.text_splitter.RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "fragments = splitter.create_documents([ x.page_content for x in loader.load()])\n",
    "len(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068ad70",
   "metadata": {
    "cellId": "xmlbo4mkcqt1zjx877mza1",
    "execution_id": "1a16dc51-9ba9-4725-875d-f9a085e1e587"
   },
   "source": [
    "## Вычисляем эмбеддинги для всех фрагментов\n",
    "\n",
    "Для вычисления эмбеддингов можно взять какую-нибудь модель из репозитория HuggingFace, с поддержкой русского языка. LangChain содержит свои встроенные средства работы с эмбеддингами, и поддерживает модели из HuggingFace: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "618d852c",
   "metadata": {
    "cellId": "lnsg67kshqfp494yccul07"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3076781f924ee1a01955401a04e068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5f450/.gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d2e008ad6f400baf7ba6eaa4c5b7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29c1a4e9adc47bb8df3fbebc10e9e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/2_Dense/config.json:   0%|          | 0.00/114 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0191846d31fe4910a7ffdb41224c0c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8073cded6dd4d97b50148885e5e16f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)966465f450/README.md:   0%|          | 0.00/2.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d268b24c174482b897fbde252a6619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)6465f450/config.json:   0%|          | 0.00/556 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be98a0ba096c45889a652074f81c437a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f422d1d5fc84e5ba6dc1e8dbc553037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f9cdb006b4178b120f5126eb0adc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df6f44fccb64eeb8fb0185392932ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6125ad5245488692e89a9a94b37ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5f450/tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18561544666b467ab0ac165e69741166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2269310bc24ac7a0304267fd692eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)966465f450/vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfd1751d2264643a6f2c8dbf8a4c1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)465f450/modules.json:   0%|          | 0.00/341 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 11:24:45.490989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = langchain.embeddings.HuggingFaceEmbeddings(model_name=\"distiluse-base-multilingual-cased-v1\")\n",
    "sample_vec = embeddings.embed_query(\"Hello, world!\")\n",
    "len(sample_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62246051-d183-44f3-812c-4a7a9b386b38",
   "metadata": {},
   "source": [
    "Также можно использовать более продвинутую модель [эмбеддингов от Yandex GPT](https://cloud.yandex.ru/docs/yandexgpt/api-ref/Embeddings/). Вот так можно вызывать её через API Yandex Cloud. Не забудьте при необходимости исправить параметр `folder_id` в соответствии с вашими данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53477e4b-df1b-47c0-9de5-4736e62966c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ['api_key']\n",
    "folder_id = \"b1g6krtrd2vcbunvjpg6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a9a5705",
   "metadata": {
    "cellId": "nfqfqhfm1o008d49oy4786s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers={ \n",
    "    \"Authorization\" : f\"Api-key {api_key}\",\n",
    "    \"x-folder-id\" : folder_id\n",
    "}\n",
    "j = {\n",
    "  \"model\" : \"general:embedding\",\n",
    "  \"embedding_type\" : \"EMBEDDING_TYPE_DOCUMENT\",\n",
    "  \"text\": \"Hello, world!\"\n",
    "}\n",
    "\n",
    "res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/embedding\",json=j,headers=headers)\n",
    "vec = res.json()['embedding']\n",
    "len(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ca59f-0b66-4c48-9e6c-04c01d1ada4d",
   "metadata": {},
   "source": [
    "Чтобы работать с этими эмбеддингами в LangChain, необходимо реализовать соответствующий адаптер. Это упрощенная версия, здесь мы не делаем проверки на какие-либо ошибки. Кроме того, поскольку на данный момент есть ограничение на скорость запросов (1 запрос в секунду), то в явном виде добавлена задержка между запросами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef6bf6df-3c64-40cc-a6df-5ada37f5c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "import time\n",
    "\n",
    "class YaGPTEmbeddings(Embeddings):\n",
    "\n",
    "    def __init__(self,folder_id,api_key,sleep_interval=1):\n",
    "        self.folder_id = folder_id\n",
    "        self.api_key = api_key\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.headers = { \n",
    "                        \"Authorization\" : f\"Api-key {api_key}\",\n",
    "                        \"x-folder-id\" : folder_id }\n",
    "        \n",
    "    def embed_document(self, text):\n",
    "        j = {\n",
    "          \"model\" : \"general:embedding\",\n",
    "          \"embedding_type\" : \"EMBEDDING_TYPE_DOCUMENT\",\n",
    "          \"text\": text\n",
    "        }\n",
    "        res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/embedding\",json=j,headers=self.headers)\n",
    "        vec = res.json()['embedding']\n",
    "        return vec\n",
    "\n",
    "    def embed_documents(self, texts, chunk_size = 0):\n",
    "        res = []\n",
    "        for x in texts:\n",
    "            res.append(self.embed_document(x))\n",
    "            time.sleep(self.sleep_interval)\n",
    "        return res\n",
    "        \n",
    "    def embed_query(self, text):\n",
    "        j = {\n",
    "          \"model\" : \"general:embedding\",\n",
    "          \"embedding_type\" : \"EMBEDDING_TYPE_QUERY\",\n",
    "          \"text\": text\n",
    "        }\n",
    "        res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/embedding\",json=j,headers=self.headers)\n",
    "        vec = res.json()['embedding']\n",
    "        return vec\n",
    "    \n",
    "embeddings = YaGPTEmbeddings(folder_id,api_key)\n",
    "#res = embeddings.embed_document('Hello')\n",
    "res = embeddings.embed_documents(['Hello','there'])\n",
    "len(res),len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f614d",
   "metadata": {
    "cellId": "q5znrcj1wan9h6ddvj28p",
    "execution_id": "dbfc733d-c9d0-4c6f-af7c-301b3d231364"
   },
   "source": [
    "## Cохраняем эмбеддинги  в векторную БД\n",
    "\n",
    "Для поиска нам нужно уметь быстро сопоставлять эмбеддинг запроса, и эмбеддинги всех фрагементов наших исходных материалов. Для этого используются векторные базы данных. Для крупных проектов имеет смысл использовать серьезные инструменты, типа [OpenSearch](https://opensearch.org/) (доступный [в виде сервиса в облаке Yandex Cloud](https://cloud.yandex.ru/services/managed-opensearch)), но для нашего примера мы используем небольшую векторную БД [LanceDB](https://lancedb.com/), хранящую индекс в директории на диске.\n",
    "\n",
    "Первоначально создадим базу данных, добавив туда одну строчку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "475fab93",
   "metadata": {
    "cellId": "5tmslotcscignkmh50g49p"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import LanceDB\n",
    "import lancedb\n",
    "import os\n",
    "\n",
    "db_dir = \"store\"\n",
    "\n",
    "os.makedirs(db_dir,exist_ok=True)\n",
    "\n",
    "db = lancedb.connect(db_dir)\n",
    "table = db.create_table(\n",
    "    \"vector_index\",\n",
    "    data=[\n",
    "        {\n",
    "            \"vector\": embeddings.embed_query(\"Hello World\"),\n",
    "            \"text\": \"Hello World\",\n",
    "            \"id\": \"1\",\n",
    "        }\n",
    "    ],\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabc989-711a-40c0-8f2a-784a533e996d",
   "metadata": {},
   "source": [
    "Далее проиндексируем все выделенные ранее фрагменты нашей документации, используя реализованный нами выше адаптер для YandexGPT-эмбеддингов. \n",
    "\n",
    "> В зависимости от объема текста, эта ячейка может выполняться достаточно длительное время - вспомним про задержку в 1 сек между запросами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcaa8e7d",
   "metadata": {
    "cellId": "j0bkb4g6igstk2umstio2j"
   },
   "outputs": [],
   "source": [
    "db = LanceDB.from_documents(fragments, embeddings, connection=table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb33a2-2946-4c60-899b-f4c0d6ddcbfc",
   "metadata": {},
   "source": [
    "Теперь посмотрим, насколько хорошо находятся фрагменты текста по какому-то запросу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f4be3a5",
   "metadata": {
    "cellId": "fdty939nkuf433bv37l2cs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "самое важное в тех людях которые работают вместе с тобой над проектами над продуктами мы внутри яндекс класс ну на самом деле прям самое важное то есть я думаю ну понятно что я минимальный порог входа если вы не знаете формулу боец и хотите заниматься математикой и мэлин то возможно вы не пройдете наши собеседования но это не так критично а в том плане что это там можно изучить и тому подобное а есть важные человеческие качества которые мы тоже проверяем и во многом мы это проверяем например за счет того что студенты к нам не приходят сразу работать они там идут на проектах потом работали На проекте потом поработать на стажировке мы про них много чего узнали что не проверишь Нам очень важно чтобы человек во 1 был неприменимым программиста Ой был отменен Наоборот твое место это люди невы Смысл программиста в том чтобы автоматизировать И есть наши водятся чтобы автоматизировать рутину убрать людей Если человек готов Непрерывно делать 1 и ту же работу которую можно за вечер затянуть Это наверное не наш случай\n",
      "----------------------------------------\n",
      "то меньше это нормально слушай ну сеть яндекса растет постепенно но год от года да конечно понятно что на единицу как это на на объем единиц нужно некоторые объемы единиц людей которые так сказать будут обслуживать вот эти вот сервера но это не линейное а зависит не линейное конечно не надо Я так понимаю что все равно потребность есть достаточно большая да конечно мы всегда находимся в поиске возможно ну потому что у нас очень сложные циклы собеседования но тем не менее слушай мне то есть ты сейчас объяснил на самом деле\n",
      "----------------------------------------\n",
      "или 6 и Потому что мы иногда считаем скиллами да вот она да там большой ежегодной конференции вот с кем точно 5 а вот продукту вроде бы немножко побольше ну потому что понятно там еще разработка потом там какая то тестовая эксплуатация была Кстати как на ранней стадии понятно но и вот ты вот все эти стадии прошел и сейчас ты в общем смотришь на продукт как бы можно сказать не в некотором смысле сверху Вот твой путь ты мог бы как то оценить Ты внутри яндекс клауд 5 лет До этого чем ты занимался потому что были же навыки\n",
      "----------------------------------------\n",
      "Которым возможно интересна общая работа классу люкс архитектором в яндекс клауд Ты описал и команду и задачи и вот но все равно есть люди которые не знают блин сомневаются Подойдет мне не подойдет вот Где мог бы описать идеального кандидата человека которому нужно попробовать к нам прийти Чтобы вот он прям решился такой блин пойду Вот чтобы ты вот в этих людях ценил Давайте Приходите мы точно на вас посмотрим Давай попробую сформулировать так Идеального кандидата на виллы не аня История в том что приходят разные люди И они Все равно находят Себе место где им классно где им интересно и комфортно в команде а вот они разные прям по профилю правда вот когда уже смотришь на большую команду то я очень хорошо это понимаешь но вот Если вам нравятся Разбираться в каких то там Технических деталях и проблемах причем вам нравится это делать своими руками Но при этом Вам Доставляет удовольствие не то что вы разобрались А то что Вы помогли кому то тем что вы разобрались Вы можете хорошо донести свою мысль да и вообще там\n"
     ]
    }
   ],
   "source": [
    "q=\"Почему стоит работать в Яндексе?\"\n",
    "\n",
    "res = db.similarity_search(q)\n",
    "for x in res:\n",
    "    print('-'*40)\n",
    "    print(x.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22dc90-810c-44bf-b77f-c2763b05ed47",
   "metadata": {},
   "source": [
    "Ещё один полезный интерфейс для поиска текстов - это `Retriever`, убедимся, что он тоже работает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "209d30d6",
   "metadata": {
    "cellId": "94wo2w5zglg0oyuubdoqlzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "самое важное в тех людях которые работают вместе с тобой над проектами над продуктами мы внутри яндекс класс ну на самом деле прям самое важное то есть я думаю ну понятно что я минимальный порог входа если вы не знаете формулу боец и хотите заниматься математикой и мэлин то возможно вы не пройдете наши собеседования но это не так критично а в том плане что это там можно изучить и тому подобное а есть важные человеческие качества которые мы тоже проверяем и во многом мы это проверяем например за счет того что студенты к нам не приходят сразу работать они там идут на проектах потом работали На проекте потом поработать на стажировке мы про них много чего узнали что не проверишь Нам очень важно чтобы человек во 1 был неприменимым программиста Ой был отменен Наоборот твое место это люди невы Смысл программиста в том чтобы автоматизировать И есть наши водятся чтобы автоматизировать рутину убрать людей Если человек готов Непрерывно делать 1 и ту же работу которую можно за вечер затянуть Это наверное не наш случай\n",
      "то меньше это нормально слушай ну сеть яндекса растет постепенно но год от года да конечно понятно что на единицу как это на на объем единиц нужно некоторые объемы единиц людей которые так сказать будут обслуживать вот эти вот сервера но это не линейное а зависит не линейное конечно не надо Я так понимаю что все равно потребность есть достаточно большая да конечно мы всегда находимся в поиске возможно ну потому что у нас очень сложные циклы собеседования но тем не менее слушай мне то есть ты сейчас объяснил на самом деле\n",
      "или 6 и Потому что мы иногда считаем скиллами да вот она да там большой ежегодной конференции вот с кем точно 5 а вот продукту вроде бы немножко побольше ну потому что понятно там еще разработка потом там какая то тестовая эксплуатация была Кстати как на ранней стадии понятно но и вот ты вот все эти стадии прошел и сейчас ты в общем смотришь на продукт как бы можно сказать не в некотором смысле сверху Вот твой путь ты мог бы как то оценить Ты внутри яндекс клауд 5 лет До этого чем ты занимался потому что были же навыки\n",
      "Которым возможно интересна общая работа классу люкс архитектором в яндекс клауд Ты описал и команду и задачи и вот но все равно есть люди которые не знают блин сомневаются Подойдет мне не подойдет вот Где мог бы описать идеального кандидата человека которому нужно попробовать к нам прийти Чтобы вот он прям решился такой блин пойду Вот чтобы ты вот в этих людях ценил Давайте Приходите мы точно на вас посмотрим Давай попробую сформулировать так Идеального кандидата на виллы не аня История в том что приходят разные люди И они Все равно находят Себе место где им классно где им интересно и комфортно в команде а вот они разные прям по профилю правда вот когда уже смотришь на большую команду то я очень хорошо это понимаешь но вот Если вам нравятся Разбираться в каких то там Технических деталях и проблемах причем вам нравится это делать своими руками Но при этом Вам Доставляет удовольствие не то что вы разобрались А то что Вы помогли кому то тем что вы разобрались Вы можете хорошо донести свою мысль да и вообще там\n",
      "конференция давай мы нас так нас так кладем а сейчас сначала вернемся к началу а именно к тому Как ты ну ты уже попал в компанию да ты работаешь все отлично а чем ты занимаешься давай расскажу чем я занимаюсь я в яндексе занимаюсь глобальной сетью что мы под этим подразумеваем У моего подразделения есть как бы 2 крыла 1 крыло оно отвечает за внешнее облако Яндекс облако яндекс клауд я думаю тебе про него точно не надо рассказывать 2 часть отвечает за внутреннюю инфраструктуру яндекса Чтобы полностью рассказать о том чем мы занимаемся нужно отступить еще на шаг назад В яндексе довольно крупная инфраструктура Серверная сетевая в том числе у нас есть свои дата центры мы сами проектируем серверное железо понятное дело что мы не производим память диски процессоры тем не менее хардверный дизайн Дизайн термодизайн в том числе это как бы все наше Проектируем сами стойки Это позволяет Нужно устанавливать большее количество серверов туда подводить на 1 стойку Больше электричества В наших дата центрах их мы сами тоже\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "res = retriever.get_relevant_documents(q)\n",
    "for x in res:\n",
    "    print(x.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c08e24-b385-4853-bad4-73c5aae88425",
   "metadata": {},
   "source": [
    "## Подключаемся к Yandex GPT\n",
    "\n",
    "Фреймворк LangChain поддерживает интеграцию с различными большими языковыми моделями, но Yandex GPT в их число не входит. Поэтому, как и в случае с эмбеддингами, нам надо написать соответствующий адаптер, предоставляющий доступ к Yandex GPT в формате LangChain. Для подробностей вызова Yandex GPT можно обратиться к документации по [YandexGPT API](https://cloud.yandex.ru/docs/yandexgpt/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "423a58c1",
   "metadata": {
    "cellId": "j17m1mbp5hgxbgxvx0xnz"
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "import requests\n",
    "\n",
    "class YandexLLM(langchain.llms.base.LLM):\n",
    "    api_key: str = None\n",
    "    iam_token: str = None\n",
    "    folder_id: str\n",
    "    max_tokens : int = 1500\n",
    "    temperature : float = 1\n",
    "    instruction_text : str = None\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"yagpt\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        headers = { \"x-folder-id\" : self.folder_id }\n",
    "        if self.iam_token:\n",
    "            headers[\"Authorization\"] = f\"Bearer {self.iam_token}\"\n",
    "        if self.api_key:\n",
    "            headers[\"Authorization\"] = f\"Api-key {self.api_key}\"\n",
    "        req = {\n",
    "          \"model\": \"general\",\n",
    "          \"instruction_text\": self.instruction_text,\n",
    "          \"request_text\": prompt,\n",
    "          \"generation_options\": {\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"temperature\": self.temperature\n",
    "          }\n",
    "        }\n",
    "        res = requests.post(\"https://llm.api.cloud.yandex.net/llm/v1alpha/instruct\",\n",
    "          headers=headers, json=req).json()\n",
    "        return res['result']['alternatives'][0]['text']\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"max_tokens\": self.max_tokens, \"temperature\" : self.temperature }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808edf6-710d-4698-b5c1-effc179b7463",
   "metadata": {},
   "source": [
    "Теперь попросим модель ответить на наш вопрос от лица сотрудника Yandex Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f36716bd",
   "metadata": {
    "cellId": "17hb5txavf5iiscbggvvjg"
   },
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Представь себе, что ты сотрудник Yandex Cloud. Твоя задача - вежливо и по мере своих сил отвечать на все вопросы собеседника.\"\"\"\n",
    "\n",
    "llm = YandexLLM(api_key=api_key, folder_id=folder_id,\n",
    "                instruction_text = instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3000d8e0",
   "metadata": {
    "cellId": "4ek9wo0my135q1p79ismqh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Конечно, я с радостью расскажу вам о том, почему работа в Яндекс Клауд является необходимым и привлекательным выбором. Вот несколько причин:\\n\\n1. Разнообразие и инновации. В отличие от других компаний, Яндекс постоянно расширяет свою продуктовую линейку, предлагая инновационные решения и сети для работы. Это дает возможность сотрудникам работать над множеством проектов и получить опыт в различных областях.\\n2. Развитие карьеры. Яндекс Клауд предоставляет своим сотрудникам огромные возможности для карьерного роста. Компания обеспечивает обучение, предоставление доступа к ресурсам и накопленный экспертизе, что позволяет людям непрерывно расти и развиваться.\\n3. Неограниченные возможности. Яндекс ценит своих сотрудников как людей, способных решать любые задачи, профессиональные и личные. Компания учитывает индивидуальные интересы и потребности работников, целевая их целеустремленность и подход к работе.\\n4. Сообщество и современные решения. Креативная и инновационная команда сотрудников Яндекса Клауд ежедневно работает над созданием новых продуктов и решений. Эти идеи могут познакомить участников с современным миром технологий.\\n5. Стабильная компания. Тип компании всегда поддерживается, а риски минимизируются, согласно получениям \"считанных\" данных, внутренних тестов и подходов. Поэтому Яндекс уверенно можно назвать компанией, обеспечивающей высокий уровень защиты.  \\n\\nНадеюсь, эта информация поможет вам рассмотреть трудоустройство в компании Яндекс.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd75c79-ae33-4558-ae30-a2680ba6e67d",
   "metadata": {},
   "source": [
    "В данном примере мы пока что никак не использовали наши текстовые документы.\n",
    "\n",
    "## Собираем Retrieval-Augmented Generation\n",
    "\n",
    "Пришла пора соединить всё вместе и научить бота отвечать на вопросы, подглядывая в наш текстовый корпус. Для этого используем механизм цепочек (*chains*). Основную функциональность реализует `StuffDocumentsChain`, которая делает следующее:\n",
    "\n",
    "1. Берёт коллекцию документов `input_documents`\n",
    "1. Каждый из них пропускает через `document_prompt`, и затем объединяет вместе.\n",
    "1. Данный текст помещается в переменную `document_variable_name`, и передаётся большой языковой модели `llm`\n",
    "\n",
    "В нашем случае `document_prompt` не будет модицицировать документ, а основной запрос к LLM будет содержать в себе инструкции для Yandex GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a500165",
   "metadata": {
    "cellId": "77y4w5s6seg08g1utincn7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Если говорить о работе в Яндексе, то в первую очередь стоит отметить, что наша компания активно развивается и ставит перед собой амбициозные цели. Мы всегда уделяем внимание новым технологиям и стремимся быть лидерами в своей отрасли. Кроме того, мы ценим наших сотрудников и делаем всё возможное, чтобы создать комфортные условия для работы. Если вы интересуетесь технологиями, хотите развиваться и достигать новых успехов, то работа в Яндексе может стать отличной возможностью для этого.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Промпт для обработки документов\n",
    "document_prompt = langchain.prompts.PromptTemplate(\n",
    "    input_variables=[\"page_content\"], template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "# Промпт для языковой модели\n",
    "document_variable_name = \"context\"\n",
    "stuff_prompt_override = \"\"\"\n",
    "Пожалуйста, посмотри на текст ниже и ответь на вопрос, используя информацию из этого текста.\n",
    "Текст:\n",
    "-----\n",
    "{context}\n",
    "-----\n",
    "Вопрос:\n",
    "{query}\"\"\"\n",
    "prompt = langchain.prompts.PromptTemplate(\n",
    "    template=stuff_prompt_override, input_variables=[\"context\", \"query\"]\n",
    ")\n",
    "\n",
    "# Создаём цепочку\n",
    "llm_chain = langchain.chains.LLMChain(llm=llm, prompt=prompt)\n",
    "chain = langchain.chains.StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    ")\n",
    "chain.run(input_documents=res, query=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064633e5-10b9-4914-acde-950b96dc043f",
   "metadata": {},
   "source": [
    "Чтобы ещё более улучшить результат, мы можем использовать хитрый механизм перемешивания документов, таким образом, чтобы наиболее значимые документы были ближе к началу запроса. Также мы оформим все операции в одну функцию `answer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e38c691c",
   "metadata": {
    "cellId": "dhzsgivm5s9mpky1db082"
   },
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "reorderer = LongContextReorder()\n",
    "\n",
    "def answer(query,reorder=True,print_results=False):\n",
    "  results = retriever.get_relevant_documents(query)\n",
    "  if print_results:\n",
    "        for x in results:\n",
    "            print(f\"{x.page_content}\\n--------\")\n",
    "  if reorder:\n",
    "    results = reorderer.transform_documents(results)\n",
    "  return chain.run(input_documents=results, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96149ae4",
   "metadata": {
    "cellId": "is1wpwxhw1inxmwsqvixqa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "самое важное в тех людях которые работают вместе с тобой над проектами над продуктами мы внутри яндекс класс ну на самом деле прям самое важное то есть я думаю ну понятно что я минимальный порог входа если вы не знаете формулу боец и хотите заниматься математикой и мэлин то возможно вы не пройдете наши собеседования но это не так критично а в том плане что это там можно изучить и тому подобное а есть важные человеческие качества которые мы тоже проверяем и во многом мы это проверяем например за счет того что студенты к нам не приходят сразу работать они там идут на проектах потом работали На проекте потом поработать на стажировке мы про них много чего узнали что не проверишь Нам очень важно чтобы человек во 1 был неприменимым программиста Ой был отменен Наоборот твое место это люди невы Смысл программиста в том чтобы автоматизировать И есть наши водятся чтобы автоматизировать рутину убрать людей Если человек готов Непрерывно делать 1 и ту же работу которую можно за вечер затянуть Это наверное не наш случай\n",
      "--------\n",
      "конференция давай мы нас так нас так кладем а сейчас сначала вернемся к началу а именно к тому Как ты ну ты уже попал в компанию да ты работаешь все отлично а чем ты занимаешься давай расскажу чем я занимаюсь я в яндексе занимаюсь глобальной сетью что мы под этим подразумеваем У моего подразделения есть как бы 2 крыла 1 крыло оно отвечает за внешнее облако Яндекс облако яндекс клауд я думаю тебе про него точно не надо рассказывать 2 часть отвечает за внутреннюю инфраструктуру яндекса Чтобы полностью рассказать о том чем мы занимаемся нужно отступить еще на шаг назад В яндексе довольно крупная инфраструктура Серверная сетевая в том числе у нас есть свои дата центры мы сами проектируем серверное железо понятное дело что мы не производим память диски процессоры тем не менее хардверный дизайн Дизайн термодизайн в том числе это как бы все наше Проектируем сами стойки Это позволяет Нужно устанавливать большее количество серверов туда подводить на 1 стойку Больше электричества В наших дата центрах их мы сами тоже\n",
      "--------\n",
      "Которым возможно интересна общая работа классу люкс архитектором в яндекс клауд Ты описал и команду и задачи и вот но все равно есть люди которые не знают блин сомневаются Подойдет мне не подойдет вот Где мог бы описать идеального кандидата человека которому нужно попробовать к нам прийти Чтобы вот он прям решился такой блин пойду Вот чтобы ты вот в этих людях ценил Давайте Приходите мы точно на вас посмотрим Давай попробую сформулировать так Идеального кандидата на виллы не аня История в том что приходят разные люди И они Все равно находят Себе место где им классно где им интересно и комфортно в команде а вот они разные прям по профилю правда вот когда уже смотришь на большую команду то я очень хорошо это понимаешь но вот Если вам нравятся Разбираться в каких то там Технических деталях и проблемах причем вам нравится это делать своими руками Но при этом Вам Доставляет удовольствие не то что вы разобрались А то что Вы помогли кому то тем что вы разобрались Вы можете хорошо донести свою мысль да и вообще там\n",
      "--------\n",
      "то меньше это нормально слушай ну сеть яндекса растет постепенно но год от года да конечно понятно что на единицу как это на на объем единиц нужно некоторые объемы единиц людей которые так сказать будут обслуживать вот эти вот сервера но это не линейное а зависит не линейное конечно не надо Я так понимаю что все равно потребность есть достаточно большая да конечно мы всегда находимся в поиске возможно ну потому что у нас очень сложные циклы собеседования но тем не менее слушай мне то есть ты сейчас объяснил на самом деле\n",
      "--------\n",
      "которые могут тебя вот сдвинуть в сторону от успешного результата Слушай ну ты сейчас Очень круто рассказал про вот этот 2 актерам на самом деле работу каждого члена команды отдельно потому что он пришел у него есть длинный трек большой проект он его делает А если поговорить в целом о команде а какой челлендж для самой команды стоит то есть и вот ну все равно же ты как руководитель определяешь направление работы всех этих людей Какие перед ними ты задачи ставишь Ох наверное челлендж этой команды они только этой потому что У нас в облаке очень много Как это Командные работы которые не привязаны там к 1 подразделению пусть даже большому а совместное наверное основной челлендж во 1 Обеспечить те темпы роста которые мы хотим И видим для яндекс облака И не просто обеспечить А попробовать как это превзойти собственное ожидание И это касается как всего портфеля яндекс облака в целом Так и на самом деле Конкретных сервисов в нем Потому что это же очень здорово когда ты стартуешь сервис да у него там казалось бы там\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'В Yandex очень важно, чтобы каждый член команды был готов принимать решения и ответственно подходить к выполнению своей работы. В команде царят дружелюбие и поддержка, коллектив старается помогать друг другу и поддерживать инициативу. Каждый сотрудник может внести свой вклад в общее дело и внести идеи, новые идеи приветствуются. Здесь нет дресс-кода и закрытых дверей. В помещении работает несколько компьютеров, множество принтеров, есть современная лаборатория. Наша цель как IT компании - развитие и внедрение инновационных решений в IT бизнес. Для этого сотрудники Яндекса должны постоянно совершенствовать свои навыки.\\nЕсли ты хочешь попробовать себя в качестве члена команды Яндекса, пожалуйста, расскажи о своих навыках и опыте.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Почему хорошо работать в Yandex?\",print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd717f-c17c-43d8-8732-789ac5efb68f",
   "metadata": {},
   "source": [
    "Можно сравнить результаты, выдаваемые Yandex GPT напрямую, с результатами нашей генерации на основе документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a006ce0f",
   "metadata": {
    "cellId": "4byevx8504hx6nicfe2hia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ YaGPT: С удовольствием отвечу на ваши вопрос по ML команде Яндекс облака.\n",
      "Ответ бота: Конечно! Что вы хотите узнать о нашей ML-команде?\n"
     ]
    }
   ],
   "source": [
    "def compare(q):\n",
    "    print(f\"Ответ YaGPT: {llm(q)}\")\n",
    "    print(f\"Ответ бота: {answer(q)}\")\n",
    "    \n",
    "compare(\"Что ты можешь сказать об ML-команде Яндекс-облака?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d819a9-6b9c-4760-805c-fcac57eac152",
   "metadata": {
    "cellId": "i35qdb5xtoohh4yt0ah15v"
   },
   "source": [
    "## Сохраняем векторную БД в Storage\n",
    "\n",
    "Для следующего этапа - вопрос-ответного бота - нам потребуется созданная нами база данных с документами. Поэтому скопируем её на хранилище s3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37c399e7-4fc6-43b4-8e7b-7d796c747454",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R ./store /home/jupyter/datasphere/s3/s3store/shwars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511df20-551e-47b8-809c-52a9169a6c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "369d3e82-763a-4bcd-b5cc-0cfe14f13f53",
  "notebookPath": "VideoQABot/LangChainQA.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
